{
  "metadata": {
    "version": "1.0",
    "title": "Map of AI Futures",
    "canvas": {
      "width": 2800,
      "height": 3600
    }
  },
  "nodes": [
    {
      "id": "sstart",
      "type": "start",
      "title": "START",
      "description": "",
      "position": {
        "x": 450,
        "y": 420
      },
      "sliderIndex": null,
      "probability": null,
      "connections": [
        {
          "targetId": "ncapabilities-plateau",
          "type": "always",
          "label": ""
        }
      ]
    },
    {
      "id": "adune",
      "type": "ambivalentOutcome",
      "title": "Advanced AI is permanently banned",
      "description": "",
      "position": {
        "x": 1550,
        "y": 420
      },
      "sliderIndex": null,
      "probability": null,
      "connections": []
    },
    {
      "id": "ncapabilities-plateau",
      "type": "question",
      "title": "Will current AI approaches plateau before AGI-level capabilities?",
      "description": "",
      "position": {
        "x": 450,
        "y": 570
      },
      "sliderIndex": 0,
      "probability": 50,
      "connections": [
        {
          "targetId": "npause",
          "type": "no",
          "label": "No plateau"
        },
        {
          "targetId": "nnew-approaches",
          "type": "yes",
          "label": "Plateau"
        }
      ]
    },
    {
      "id": "nnew-approaches",
      "type": "question",
      "title": "Will new approaches unlock AGI-level capabilities?",
      "description": "",
      "position": {
        "x": 700,
        "y": 570
      },
      "sliderIndex": 1,
      "probability": 70,
      "connections": [
        {
          "targetId": "npause",
          "type": "yes",
          "label": "Unlocks AGI"
        },
        {
          "targetId": "aai-winter",
          "type": "no",
          "label": "No unlock"
        }
      ]
    },
    {
      "id": "aai-winter",
      "type": "ambivalentOutcome",
      "title": "Permanent AI winter",
      "description": "",
      "position": {
        "x": 700,
        "y": 720
      },
      "sliderIndex": null,
      "probability": null,
      "connections": []
    },
    {
      "id": "npause",
      "type": "question",
      "title": "Will we pause advancing AI capabilities before they reach catastrophic potential?",
      "description": "",
      "position": {
        "x": 700,
        "y": 420
      },
      "sliderIndex": 2,
      "probability": 25,
      "connections": [
        {
          "targetId": "ipause",
          "type": "yes",
          "label": "Pause"
        },
        {
          "targetId": "ireach-catastrophic-potential",
          "type": "no",
          "label": "No pause"
        }
      ]
    },
    {
      "id": "ipause",
      "type": "intermediateStep",
      "title": "Pause allows more safety research and other precautions",
      "description": "",
      "position": {
        "x": 1000,
        "y": 420
      },
      "sliderIndex": null,
      "probability": null,
      "connections": [
        {
          "targetId": "ncontinue",
          "type": "always",
          "label": ""
        }
      ]
    },
    {
      "id": "ncontinue",
      "type": "question",
      "title": "Will we continue AI capabilities research after a pause?",
      "description": "",
      "position": {
        "x": 1300,
        "y": 420
      },
      "sliderIndex": 3,
      "probability": 96,
      "connections": [
        {
          "targetId": "ireach-catastrophic-potential",
          "type": "yes",
          "label": "Continue"
        },
        {
          "targetId": "adune",
          "type": "no",
          "label": "No continue"
        }
      ]
    },
    {
      "id": "ireach-catastrophic-potential",
      "type": "intermediateStep",
      "title": "Frontier AI reaches capabilities with catastrophic potential",
      "description": "",
      "position": {
        "x": 1000,
        "y": 570
      },
      "sliderIndex": null,
      "probability": null,
      "connections": [
        {
          "targetId": "ncatastrophe",
          "type": "always",
          "label": ""
        }
      ]
    },
    {
      "id": "ncatastrophe",
      "type": "question",
      "title": "Does a pre-AGI catastrophe occur (e.g. AI misuse via cyber- or bio-weapons)?",
      "description": "",
      "position": {
        "x": 1300,
        "y": 720
      },
      "sliderIndex": 4,
      "probability": 55,
      "connections": [
        {
          "targetId": "icatastrophy-occurs",
          "type": "yes",
          "label": "Catastrophe"
        },
        {
          "targetId": "iagi-exists",
          "type": "no",
          "label": "No catastrophe"
        }
      ]
    },
    {
      "id": "icatastrophy-occurs",
      "type": "intermediateStep",
      "title": "Pre-AGI catastrophe does occur!",
      "description": "",
      "position": {
        "x": 1300,
        "y": 570
      },
      "sliderIndex": null,
      "probability": null,
      "connections": [
        {
          "targetId": "nstop",
          "type": "always",
          "label": ""
        }
      ]
    },
    {
      "id": "nstop",
      "type": "question",
      "title": "Humanity permanently fully stops frontier AI development (after catastrophe)?",
      "description": "",
      "position": {
        "x": 1550,
        "y": 720
      },
      "sliderIndex": 5,
      "probability": 5,
      "connections": [
        {
          "targetId": "iagi-exists",
          "type": "no",
          "label": "Doesn't stop"
        },
        {
          "targetId": "adune",
          "type": "yes",
          "label": "Stops development"
        }
      ]
    },
    {
      "id": "iagi-exists",
      "type": "intermediateStep",
      "title": "Research towards AGI continues. First AGI exists",
      "description": "",
      "position": {
        "x": 1550,
        "y": 870
      },
      "sliderIndex": null,
      "probability": null,
      "connections": [
        {
          "targetId": "nagi-transformative",
          "type": "always",
          "label": ""
        }
      ]
    },
    {
      "id": "nagi-transformative",
      "type": "question",
      "title": "Will AGI become transformative?",
      "description": "",
      "position": {
        "x": 1300,
        "y": 870
      },
      "sliderIndex": 6,
      "probability": 95,
      "connections": [
        {
          "targetId": "nalignment-theory",
          "type": "yes",
          "label": "Transformative"
        },
        {
          "targetId": "anon-transformative-agi",
          "type": "no",
          "label": "Non-transformative"
        }
      ]
    },
    {
      "id": "anon-transformative-agi",
      "type": "ambivalentOutcome",
      "title": "Powerful, but not transformative AGI",
      "description": "",
      "position": {
        "x": 1000,
        "y": 870
      },
      "sliderIndex": null,
      "probability": null,
      "connections": []
    },
    {
      "id": "nalignment-theory",
      "type": "question",
      "title": "Do we have a practical solution for AGI alignment?",
      "description": "",
      "position": {
        "x": 1300,
        "y": 1020
      },
      "sliderIndex": 7,
      "probability": 75,
      "connections": [
        {
          "targetId": "nimplement-alignment1",
          "type": "yes",
          "label": "Solution exists"
        },
        {
          "targetId": "imisaligned-agi",
          "type": "no",
          "label": "No solution"
        }
      ]
    },
    {
      "id": "nimplement-alignment1",
      "type": "question",
      "title": "Will the first AGI lab implement alignment correctly?",
      "description": "",
      "position": {
        "x": 1000,
        "y": 1020
      },
      "sliderIndex": 8,
      "probability": 65,
      "connections": [
        {
          "targetId": "imisaligned-agi",
          "type": "no",
          "label": "Misaligned first AGI"
        },
        {
          "targetId": "nprevent-other-agis",
          "type": "yes",
          "label": "Aligned first AGI"
        }
      ]
    },
    {
      "id": "nprevent-other-agis",
      "type": "question",
      "title": "Will the first AGI prevent other AGIs from appearing?",
      "description": "",
      "position": {
        "x": 700,
        "y": 1020
      },
      "sliderIndex": 9,
      "probability": 40,
      "connections": [
        {
          "targetId": "nimplement-alignment2",
          "type": "no",
          "label": "Doesn't prevent"
        },
        {
          "targetId": "ialigned-agi",
          "type": "yes",
          "label": "Prevents others"
        }
      ]
    },
    {
      "id": "nimplement-alignment2",
      "type": "question",
      "title": "Will all other AGI labs implement alignment correctly?",
      "description": "",
      "position": {
        "x": 700,
        "y": 1170
      },
      "sliderIndex": 10,
      "probability": 65,
      "connections": [
        {
          "targetId": "imultipolar",
          "type": "no",
          "label": "Others misaligned"
        },
        {
          "targetId": "ialigned-agi",
          "type": "yes",
          "label": "Others aligned"
        }
      ]
    },
    {
      "id": "imultipolar",
      "type": "intermediateStep",
      "title": "Multiple conflicting AGIs exist",
      "description": "",
      "position": {
        "x": 1000,
        "y": 1170
      },
      "sliderIndex": null,
      "probability": null,
      "connections": [
        {
          "targetId": "imisaligned-agi",
          "type": "always",
          "label": ""
        }
      ]
    },
    {
      "id": "ialigned-agi",
      "type": "intermediateStep",
      "title": "All AGIs are aligned",
      "description": "",
      "position": {
        "x": 450,
        "y": 1170
      },
      "sliderIndex": null,
      "probability": null,
      "connections": [
        {
          "targetId": "nwhole-humanity-aligned",
          "type": "always",
          "label": ""
        }
      ]
    },
    {
      "id": "nwhole-humanity-aligned",
      "type": "question",
      "title": "Is AGI aligned with humanity's interests as a whole?",
      "description": "",
      "position": {
        "x": 450,
        "y": 1320
      },
      "sliderIndex": 11,
      "probability": 20,
      "connections": [
        {
          "targetId": "ncontrollers-thoughtful",
          "type": "no",
          "label": "Controller-aligned"
        },
        {
          "targetId": "gagi-utopia2",
          "type": "yes",
          "label": "Humanity-aligned"
        }
      ]
    },
    {
      "id": "gagi-utopia2",
      "type": "goodOutcome",
      "title": "AGI utopia",
      "description": "",
      "position": {
        "x": 700,
        "y": 1320
      },
      "sliderIndex": null,
      "probability": null,
      "connections": []
    },
    {
      "id": "ncontrollers-thoughtful",
      "type": "question",
      "title": "Are AGI controllers wise enough to avoid unintended consequences?",
      "description": "",
      "position": {
        "x": 450,
        "y": 1470
      },
      "sliderIndex": 12,
      "probability": 60,
      "connections": [
        {
          "targetId": "nagi-protects",
          "type": "no",
          "label": "Unwise controllers"
        },
        {
          "targetId": "ncontrollers-good",
          "type": "yes",
          "label": "Wise controllers"
        }
      ]
    },
    {
      "id": "nagi-protects",
      "type": "question",
      "title": "Does AGI protect its users?",
      "description": "",
      "position": {
        "x": 450,
        "y": 1620
      },
      "sliderIndex": 13,
      "probability": 75,
      "connections": [
        {
          "targetId": "ncontrollers-good",
          "type": "yes",
          "label": "Protects users"
        },
        {
          "targetId": "estupid-xrisk",
          "type": "no",
          "label": "Doesn't protect"
        }
      ]
    },
    {
      "id": "estupid-xrisk",
      "type": "existentialOutcome",
      "title": "Accidental human extinction",
      "description": "",
      "position": {
        "x": 450,
        "y": 1770
      },
      "sliderIndex": null,
      "probability": null,
      "connections": []
    },
    {
      "id": "ncontrollers-good",
      "type": "question",
      "title": "Do AGI controllers have good intentions?",
      "description": "",
      "position": {
        "x": 700,
        "y": 1470
      },
      "sliderIndex": 14,
      "probability": 75,
      "connections": [
        {
          "targetId": "eauthoritarian-dystopia",
          "type": "no",
          "label": "Bad intentions"
        },
        {
          "targetId": "gagi-utopia2",
          "type": "yes",
          "label": "Good intentions"
        }
      ]
    },
    {
      "id": "eauthoritarian-dystopia",
      "type": "existentialOutcome",
      "title": "Authoritarian dystopia",
      "description": "",
      "position": {
        "x": 700,
        "y": 1620
      },
      "sliderIndex": null,
      "probability": null,
      "connections": []
    },
    {
      "id": "imisaligned-agi",
      "type": "intermediateStep",
      "title": "(At least one) Misaligned AGI",
      "description": "",
      "position": {
        "x": 1300,
        "y": 1170
      },
      "sliderIndex": null,
      "probability": null,
      "connections": [
        {
          "targetId": "nfigure-misaligned",
          "type": "always",
          "label": ""
        }
      ]
    },
    {
      "id": "nfigure-misaligned",
      "type": "question",
      "title": "Do we figure out right away that it's misaligned?",
      "description": "",
      "position": {
        "x": 1000,
        "y": 1320
      },
      "sliderIndex": 15,
      "probability": 10,
      "connections": [
        {
          "targetId": "nturn-off",
          "type": "yes",
          "label": "Figure it out"
        },
        {
          "targetId": "nharming-us",
          "type": "no",
          "label": "Don't figure it out"
        }
      ]
    },
    {
      "id": "nturn-off",
      "type": "question",
      "title": "Can we easily turn it off?",
      "description": "",
      "position": {
        "x": 1300,
        "y": 1320
      },
      "sliderIndex": 16,
      "probability": 30,
      "connections": [
        {
          "targetId": "asecond-chance",
          "type": "yes",
          "label": "Can turn off"
        },
        {
          "targetId": "nharming-us",
          "type": "no",
          "label": "Cannot turn off"
        }
      ]
    },
    {
      "id": "asecond-chance",
      "type": "ambivalentOutcome",
      "title": "We get a second chance at building AGI",
      "description": "",
      "position": {
        "x": 1550,
        "y": 1320
      },
      "sliderIndex": null,
      "probability": null,
      "connections": []
    },
    {
      "id": "nharming-us",
      "type": "question",
      "title": "Does it pursue actions that harm us?",
      "description": "",
      "position": {
        "x": 1300,
        "y": 1470
      },
      "sliderIndex": 17,
      "probability": 95,
      "connections": [
        {
          "targetId": "ncan-destroy-it",
          "type": "yes",
          "label": "Harms us"
        },
        {
          "targetId": "asecond-chance",
          "type": "no",
          "label": "Doesn't harm"
        }
      ]
    },
    {
      "id": "ncan-destroy-it",
      "type": "question",
      "title": "Can we destroy it?",
      "description": "",
      "position": {
        "x": 1550,
        "y": 1470
      },
      "sliderIndex": 18,
      "probability": 3,
      "connections": [
        {
          "targetId": "asecond-chance",
          "type": "yes",
          "label": "Can destroy"
        },
        {
          "targetId": "nbenefit-extinction",
          "type": "no",
          "label": "Cannot destroy"
        }
      ]
    },
    {
      "id": "nbenefit-extinction",
      "type": "question",
      "title": "Does it benefit from our extinction?",
      "description": "",
      "position": {
        "x": 1550,
        "y": 1620
      },
      "sliderIndex": 19,
      "probability": 97,
      "connections": [
        {
          "targetId": "eagi-kills-us",
          "type": "yes",
          "label": "Benefits from extinction"
        },
        {
          "targetId": "nbenefit-suffering",
          "type": "no",
          "label": "No benefit"
        }
      ]
    },
    {
      "id": "eagi-kills-us",
      "type": "existentialOutcome",
      "title": "AGI-initiated human extinction",
      "description": "",
      "position": {
        "x": 1550,
        "y": 1770
      },
      "sliderIndex": null,
      "probability": null,
      "connections": []
    },
    {
      "id": "nbenefit-suffering",
      "type": "question",
      "title": "Does it benefit from our suffering?",
      "description": "",
      "position": {
        "x": 1300,
        "y": 1620
      },
      "sliderIndex": 20,
      "probability": 4,
      "connections": [
        {
          "targetId": "esuffering",
          "type": "yes",
          "label": "Benefits from suffering"
        },
        {
          "targetId": "emedium-dystopia",
          "type": "no",
          "label": "No benefit"
        }
      ]
    },
    {
      "id": "emedium-dystopia",
      "type": "existentialOutcome",
      "title": "AI dystopia: humans survive, but AI is in control",
      "description": "",
      "position": {
        "x": 1300,
        "y": 1770
      },
      "sliderIndex": null,
      "probability": null,
      "connections": []
    },
    {
      "id": "esuffering",
      "type": "existentialOutcome",
      "title": "Astronomical suffering",
      "description": "",
      "position": {
        "x": 1000,
        "y": 1620
      },
      "sliderIndex": null,
      "probability": null,
      "connections": []
    }
  ]
}